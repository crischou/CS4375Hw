---
title: "Homework 3"
subtitle: "4375 Machine Learning with Dr. Mazidi"
author: "Cris Chou"
date: "9/14"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

This homework runs logistic regression to predict the binary feature of whether or not a person was admitted to graduate school, based on a set of predictors: GRE score, TOEFL score, rating of undergrad university attended, SOP statement of purpose, LOR letter or recommendation, Undergrad GPA, Research experience (binary).

The data set was downloaded from Kaggle: https://www.kaggle.com/mohansacharya/graduate-admissions

The data is available in Piazza. 

## Step 1 Load the data

* Load the data
* Examine the first few rows with head()

```{r}
# your code here
df <- read.csv("Admission_Predict.csv")
head(df)

```

## Step 2 Data Wrangling

Perform the following steps:

* Make Research a factor
* Get rid of the Serial No column
* Make a new column that is binary factor based on if Chance.of.Admit > 0.5. Hint: See p. 40 in the book. 
* Output column names with names() function
* Output a summary of the data
* Is the data set unbalanced? Why or why not?

 Your commentary here:
 The data set is unbalanced because a disproportionate percent of the dataset had a higher than .5 chance of getting admitted. In a total of 400 data points there are only 35 points that didn't have a higher than .5 chance of getting admitted. 
 

```{r}
# your code here
df$Research <- factor(df$Research)
df$Serial.No. <- NULL
df$Admit <- FALSE
df$Admit[df$Chance.of.Admit > .5] <- TRUE 
df$Admit <- factor(df$Admit)
names(df)


```

```{r}
# put the summary here
summary(df)
```

## Step 3 Data Visualization

* Create a side-by-side graph with Admit on the x axis of both graphs, GRE score on the y axis of one graph and TOEFL score on the y axis of the other graph; save/restore the original graph parameters
* Comment on the graphs and what they are telling you about whether GRE and TOEFL are good predictors
* You will get a lot of warnings, you can suppress them with disabling warnings as shown below:

```
{r,warning=FALSE}
```

Your commentary here:

```{r,warning=FALSE}
# your code here
par(mfrow=c(1,2))
plot(df$GRE.Score~df$Admit, xlab = "Admit > .5", ylab = "GRE Score",varwidth=TRUE)
plot(df$TOEFL.Score~df$Admit, xlab = "Admit > .5", ylab = "TOEFL Score",varwidth=TRUE)

```


## Step 4 Divide train/test

* Divide into 75/25 train/test, using seed 1234

```{r}
# your code here
set.seed(1234)
i <- sample(1:nrow(df)*0.75,replace=FALSE)
train <- df[i,]
test <- df[-i,]

```

## Step 5 Build a Model with all predictors 

* Build a model, predicting Admit from all predictors
* Output a summary of the model
* Did you get an error? Why? Hint: see p. 120 Warning

Your commentary here: 
Yes I got 2 errors Warning: glm.fit: algorithm did not converge
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred. This is because the training data is too perfect or almost linearly perfect. This is because all the predictors were used.


```{r}
# your code here

#glm1 <- glm(Admit~GRE.Score+TOEFL.Score+University.Rating+SOP+LOR+CGPA+Research+Chance.of.Admit, data=train, family=binomial)
glm1 <- glm(Admit~. ,data = train, family = "binomial")
summary(glm1)


```

## Step 6 Build a Model with all predictors except Chance.of.Admit

* Build another model, predicting Admit from all predictors *except* Chance.of.Admit
* Output a summary of the model
* Did you get an error? Why or why not?

There was no error since we didn't include the Chance.of.Admit predictor.

```{r}
# your code here
glm2 <- glm(Admit~.-Chance.of.Admit, data=train, family=binomial)
summary(glm2)
```

## Step 7 Predict probabilities

* Predict the probabilities using type="response"
* Examine a few probabilities and the corresponding Chance.of.Admit values
* Run cor() on the predicted probs and the Chance.of.Admit, and output the correlation
* What do you conclude from this correlation. 

Your commentary here:

```{r}
# your code here
library(ROCR)
probs <- predict(glm1, newdata=test, type="response")
pr <- prediction(probs,test$Admit)
corr <- cor(probs,test$Chance.of.Admit)
print(corr)


```

## Step 8 Make binary predictions, print table and accuracy

* Run predict() again, this time making binary predictions
* Output a table comparing the predictions and the binary Admit column
* Calculate and output accuracy
* Was the model able to generalize well to new data?

Your commentary here:
Yes it was. It shows that the model is very accurate and that it only predicted incorrectly once where it predicted the Admit to be higher than .5 when it wasn't.

```{r}
# your code here
pred <- ifelse(probs > .5, 2,1)
acc1 <- mean(pred==as.integer(test$Admit))
print(paste("glm1 accuracy = ",acc1))
table(pred,as.integer(test$Admit))

```

## Step 9 Output ROCR and AUC

* Output a ROCR graph
* Extract and output the AUC metric

```{r}
# your code here
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
print(auc)

```


## Step 10

* Make two more graphs and comment on what you learned from each graph:
  * Admit on x axis, SOP on y axis
  * Research on x axis, SOP on y axis
  
Your commentary here:
For both having a higher chance at admission and having more research both correlate to having higher SOP. 


```{r}
# plot 1
plot(df$SOP~df$Admit,xlab = "Admit > .5", ylab = "SOP" )
```

```{r}
# plot 2

plot(df$SOP~df$Research,xlab = "Research", ylab = "SOP" )
```

