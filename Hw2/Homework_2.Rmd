---
title: "Homework 2"
subtitle: "4375 Machine Learning with Dr. Mazidi"
author: "Cris Chou"
date: "9/8/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

This homework gives practice in using linear regression in two parts:

* Part 1 Simple Linear Regression (one predictor)
* Part 2 Multiple Linear Regression (many predictors)

You will need to install package ISLR at the console, not in your script. 

# Problem 1: Simple Linear Regression

## Step 1: Initial data exploration

* Load library ISLR (install.packages() at console if needed)
* Use names() and summary() to learn more about the Auto data set
* Divide the data into 75% train, 25% test, using seed 1234

```{r}
# your code here

#install.packages("ISLR")
df <- ISLR::Auto 
names(df)
summary(df)
set.seed(1234)
i <- sample(1:nrow(df), nrow(df)*0.75, replace=FALSE)
train <- df[i,]
test <- df[-i,]
```

## Step 2: Create and evaluate a linear model

* Use the lm() function to perform simple linear regression on the train data with mpg as the response and horsepower as the predictor
* Use the summary() function to evaluate the model 
* Calculate the MSE by extracting the residuals from the model like this: 
  mse <- mean(lm1$residuals^2)
* Print the MSE
* Calculate and print the RMSE by taking the square root of MSE

```{r}
# your code here
lm1 <- lm(mpg~horsepower, data=train)
summary(lm1)
mse <- mean(lm1$residuals^2)
print(mse)
rmse <- sqrt(mse)
print(rmse)
```

## Step 3 (No code. Write your answers in white space)

* Write the equation for the model, y = wx + b, filling in the parameters w, b and variable names x, y
* Is there a strong relationship between horsepower and mpg? 
*	Is it a positive or negative correlation? 
*	Comment on the RSE, R^2, and F-statistic, and how each indicates the strength of the model
*	Comment on the RMSE and whether it indicates that a good model was created

Equation: y = 39.648595+ x * -.156681 where x is the mpg and y will be the predicted horsepower
Correlation: There is a negative correlation
the RSE, R^2 don't show a strong correlation since the R^2 is a bit low. The F statistic shows that it is still statistically significant since the P value is low and the F statistic is greater than 1. 
The RMSE shows that we were off by on average around 4.853 units of horsepower which shows that the created model is ok but not the best. 

## Step 4: Examine the model graphically

* Plot train\$mpg~train\$horsepower
* Draw a blue abline()
* Comment on how well the data fits the line
* Predict mpg for horsepower of 98. Hint: See the Quick Reference 5.10.3 on page 96
* Comment on the predicted value given the graph you created

Your commentary here:
The predicted value 24.29381 is near the actual values of vehicles with a horsepower of 98. Looking at the graph at 98, the predictor line has many values near it and not too many values that are extremely far away.

```{r}
# your code here

plot(train$mpg~train$horsepower,xlab = "horsepower", ylab = "mpg")
abline(lm(train$mpg~train$horsepower), pch=19,col= "blue")
pred <- predict(lm1,data.frame(horsepower=98))
print(pred)


```

## Step 5: Evaluate on the test data

* Test on the test data using the predict function
* Find the correlation between the predicted values and the mpg values in the test data
* Print the correlation
* Calculate the mse on the test results
* Print the mse
* Compare this to the mse for the training data
* Comment on the correlation and the mse in terms of whether the model was able to generalize well to the test data

Your commentary here:

```{r}
# your code here

pred2 <- predict(lm1,newdata=test)
corr <- cor(pred2,test$mpg)
print(corr)
lm2 <- lm(mpg~horsepower,data=test)
mse2 <- mean(lm2$residuals^2)
print(mse2)
rmse2 <- sqrt(mse2)
print(rmse2)

```

## Step 6: Plot the residuals

* Plot the linear model in a 2x2 arrangement
* Do you see evidence of non-linearity from the residuals?

Your commentary here:
No, the patterns are mostly random which show that the linear model can be a good fit. 



```{r}
# your code here
par(mfrow=c(2,2))
plot(lm2)


```

## Step 7: Create a second model

* Create a second linear model with log(mpg) predicted by horsepower
* Run summary() on this second model
* Compare the summary statistic R^2 of the two models

Your commentary here:
The R^2 for this linear model(lm3) is higher than both lm1 and lm2. 

```{r}
# your code here
logMpg <- log(train$mpg)
lm3 <- lm(logMpg~train$horsepower)
summary(lm3)
summary(lm2)

```

## Step 8: Evaluate the second model graphically

* Plot log(train\$mpg)~train\$horsepower
* Draw a blue abline() 
* Comment on how well the line fits the data compared to model 1 above

Your commentary here:
This line fits better than the one from model 1. 


```{r}
# your code here
plot(logMpg~train$horsepower,xlab = "horsepower", ylab = "mpg")
abline(lm(logMpg~train$horsepower), pch=19,col= "blue")


```

## Step 9: Predict and evaluate on the second model

* Predict on the test data using lm2
* Find the correlation of the predictions and log() of test mpg, remembering to compare pred with log(test$mpg)
* Output this correlation
* Compare this correlation with the correlation you got for model 
* Calculate and output the MSE for the test data on lm2, and compare to model 1. Hint: Compute the residuals and mse like this:
```
residuals <- pred - log(test$mpg)
mse <- mean(residuals^2)
```

Your commentary here: 
The correlation is noticeably higher than the other models. The MSE is much higher than model 1. 


```{r}
# your code here
pred3 <- predict(lm2,newdata=test)
corr2 <- cor(pred3,log(test$mpg))
print(corr2)
residuals <- pred3 -log(test$mpg)
mse3 <- mean(residuals^2)
print(mse3)
rmse3 <- sqrt(mse3)
print(rmse3)



```

## Step 10: Plot the residuals of the second model

* Plot the second linear model in a 2x2 arrangement
* How does it compare to the first set of graphs?

Your commentary here:
The second set is much better than the first set since the points are more clustered together Visibly seen by having much more concentrated areas resulting in darker, more filled areas on each graph. 

```{r}
# your code here
par(mfrow=c(2,2))
plot(lm3)


```

# Problem 2: Multiple Linear Regression

## Step 1: Data exploration

* Produce a scatterplot matrix of correlations which includes all the variables in the data set using the command “pairs(Auto)”
* List any possible correlations that you observe, listing positive and negative correlations separately, with at least 3 in each category.

Your commentary here:
Positive correlations: (horsepower, weight), (weight, displacement), (displacement, horsepower)
Negative correlations: (displacement, mpg), (horsepower, acceleration), (weight, mpg)


```{r}  
# your code here
pairs(df)


```


## Step 2: Data visualization

* Display the matrix of correlations between the variables using function cor(), excluding the “name” variable since is it qualitative
* Write the two strongest positive correlations and their values below. Write the two strongest negative correlations and their values as well.

Your commentary here:
Strongest positive correlations: (weight, displacement), (displacement, cylinders)
Strongest negative correlation:  (displacement, mpg), (mpg, weight)


```{r}  
# your code here

cor(df[,c(1:8)])

```


## Step 3: Build a third linear model

* Convert the origin variable to a factor
* Use the lm() function to perform multiple linear regression with mpg as the response and all other variables except name as predictors
* Use the summary() function to print the results
* Which predictors appear to have a statistically significant relationship to the response?

Your commentary here:
The ones with statical significance seem to be weight, year, origins, and displacement. 

```{r} 
# your code here


df$origin <- factor(df$origin)
levels(df$origin) <- c("American","European","Japanese")

lm4 <-  lm(mpg~(cylinders+displacement+horsepower+weight+acceleration+year+origin),data=df)
summary(lm4)
```


## Step 4: Plot the residuals of the third model

* Use the plot() function to produce diagnostic plots of the linear regression fit
* Comment on any problems you see with the fit
* Are there any leverage points? 
* Display a row from the data set that seems to be a leverage point. 

Your commentary here:
Although all the points are close to the line of fit, the patterns for the residuals aren't too random. The leverage points seem to be 320, 323, 327, and 394. 

```{r}  
# your code here
par(mfrow=c(2,2))
plot(lm4)
df[c(320,323,327,394),]

```


## Step 5: Create and evaluate a fourth model

* Use the * and + symbols to fit linear regression models with interaction effects, choosing whatever variables you think might get better results than your model in step 3 above
* Compare the summaries of the two models, particularly R^2
* Run anova() on the two models to see if your second model outperformed the previous one, and comment below on the results

Your commentary here: 
My model(lm5) outperformed the previous model(lm4). The R^2 of my model(lm5) was .8807 compared to the previous models' .8242. The results of anova also show a smaller Res.df and RSS. 


```{r}  
# your code here
lm5 <- lm(mpg~(weight+year+origin+displacement+weight*year*origin*displacement), data=df)
summary(lm5)
summary(lm4)
anova(lm5,lm4)




```

